# Local RAG WebApp

A **completely local** Retrieval-Augmented Generation (RAG) web application for document Q&A. Upload your documents, ask questions, and get accurate answersâ€”all without any cloud dependencies.

## âœ¨ Features

- **ğŸ”’ 100% Local**: No cloud APIs, no data leaves your machine
- **ğŸ“„ Multi-format Support**: PDF, DOCX, TXT, MD, EPUB documents
- **ğŸ§  Intelligent Processing**: Adaptive chunking and dynamic-k retrieval
- **âš¡ Real-time Streaming**: Live token streaming with animated indicators
- **ğŸ¯ Accurate Citations**: See exactly which documents informed each answer
- **ğŸ”§ Performance Profiles**: Eco/Balanced/Performance modes for your hardware
- **ğŸ¨ Modern UI**: React + TypeScript frontend with accessibility support

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.9+**
- **Node.js 18+** 
- **Docker** (for Qdrant vector database)
- **Ollama** (for local LLM) - [Install here](https://ollama.ai/)

### 1. Clone and Setup

```bash
git clone <repository-url>
cd RAG_APP
```

### 2. Start Vector Database

```bash
cd docker
docker-compose up qdrant -d
```

### 3. Setup Backend

```bash
cd backend
pip install -e .

# Start Ollama and pull a model
ollama serve &
ollama pull llama3.2:3b

# Start the backend
python -m app.main
```

### 4. Setup Frontend

```bash
cd frontend
npm install
npm run dev
```

### 5. Access the Application

Open your browser to **http://localhost:5173** (Vite dev server) or **http://localhost:8000** (production build).

## ğŸ“Š Performance Profiles

Choose the profile that matches your hardware:

| Profile | CPU Usage | RAM Usage | Accuracy | Best For |
|---------|-----------|-----------|----------|----------|
| **Eco** | Low | ~2GB | Good | Battery life, older laptops |
| **Balanced** | Medium | ~4GB | Better | Most users, daily use |
| **Performance** | High | ~8GB | Best | Powerful machines, accuracy-critical |

Set via environment variable:
```bash
export RAG_PROFILE=balanced  # eco|balanced|performance
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React UI      â”‚â—„â”€â”€â–ºâ”‚  FastAPI + WS   â”‚â—„â”€â”€â–ºâ”‚    Qdrant       â”‚
â”‚   (Frontend)    â”‚    â”‚   (Backend)     â”‚    â”‚ (Vector Store)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚                       â–¼                       â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
         â”‚              â”‚     Ollama      â”‚              â”‚
         â”‚              â”‚  (Local LLM)    â”‚              â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        Local Processing Only
```

### Core Components

- **Frontend**: React 18 + TypeScript + Vite + Tailwind CSS
- **Backend**: Python FastAPI + WebSocket streaming
- **Vector Store**: Qdrant (local) with payload filters
- **LLM**: Ollama integration (llama3.2, qwen2.5, etc.)
- **Embeddings**: Local sentence-transformers models

## ğŸ“ Project Structure

```
RAG_APP/
â”œâ”€â”€ docs/                    # Specifications (URD, SRS, HLD, LLD, UI Spec)
â”œâ”€â”€ backend/                 # Python FastAPI backend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py         # FastAPI application
â”‚   â”‚   â”œâ”€â”€ api.py          # REST endpoints
â”‚   â”‚   â”œâ”€â”€ ws.py           # WebSocket streaming
â”‚   â”‚   â”œâ”€â”€ models.py       # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ settings.py     # Configuration management
â”‚   â”‚   â”œâ”€â”€ storage.py      # File operations
â”‚   â”‚   â”œâ”€â”€ parsing.py      # Document text extraction
â”‚   â”‚   â”œâ”€â”€ chunking.py     # Adaptive chunking
â”‚   â”‚   â”œâ”€â”€ embeddings.py   # Local embeddings
â”‚   â”‚   â”œâ”€â”€ qdrant_index.py # Vector store operations
â”‚   â”‚   â”œâ”€â”€ retrieval.py    # RAG + dynamic-k
â”‚   â”‚   â”œâ”€â”€ llm.py          # LLM inference
â”‚   â”‚   â””â”€â”€ diagnostics.py  # Logging & monitoring
â”‚   â””â”€â”€ tests/
â”œâ”€â”€ frontend/                # React TypeScript frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # UI components
â”‚   â”‚   â”œâ”€â”€ lib/            # API clients
â”‚   â”‚   â””â”€â”€ types.ts        # TypeScript types
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml  # Qdrant service
â””â”€â”€ README.md
```

## ğŸ› ï¸ Configuration

Create a `.env` file in the backend directory:

```bash
# Performance profile
RAG_PROFILE=balanced

# Data directory (default: ~/RAGApp)
RAG_DATA_DIR=~/RAGApp

# Services
QDRANT_URL=http://localhost:6333
OLLAMA_HOST=http://localhost:11434

# Models
RAG_LLM_MODEL=llama3.2:3b
RAG_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# RAG parameters
RAG_CHUNK_SIZE=800
RAG_CHUNK_OVERLAP=200
RAG_MAX_CONTEXT_TOKENS=4000

# Optional: Enable debug logging
RAG_DEBUG=false
```

## ğŸ§ª Advanced Features

### Adaptive Chunking
Documents are automatically chunked with optimal size (500-1350 tokens) based on:
- Content density and structure
- Presence of headings and tables
- Document type and length

### Dynamic-k Retrieval
The system automatically determines how many chunks to retrieve (3-10) based on:
- Query complexity and scope
- Marginal relevance scores
- Context budget constraints

### Real-time Streaming
Responses stream token-by-token via WebSocket with:
- Animated typing indicators
- Inline citation insertion
- Accessibility support (screen readers)

## ğŸ”§ Development

### Backend Development
```bash
cd backend

# Install with dev dependencies
pip install -e ".[dev]"

# Run tests
pytest

# Code formatting
black app/ tests/
isort app/ tests/
mypy app/

# Start with auto-reload
uvicorn app.main:app --reload
```

### Frontend Development
```bash
cd frontend

# Install dependencies
npm install

# Start dev server
npm run dev

# Run tests
npm test

# Build for production
npm run build
```

## ğŸ”’ Security & Privacy

- **Local Processing**: All operations happen on your machine
- **No Network Calls**: Application works completely offline
- **Encrypted Storage**: Optional document encryption at rest
- **Secure Deletion**: Overwrite files before deletion
- **No Telemetry**: Zero data collection or tracking

## ğŸ“Š Resource Usage

Typical usage on a modern laptop:

| Operation | CPU | RAM | Time |
|-----------|-----|-----|------|
| Document Upload (10MB PDF) | 30% | +500MB | 15s |
| Embedding Generation | 60% | +200MB | 5s |
| Query Processing | 45% | +300MB | 2s |
| LLM Response (100 tokens) | 40% | +400MB | 8s |

## ğŸ› Troubleshooting

### Common Issues

**Qdrant connection failed**
```bash
# Check if Qdrant is running
curl http://localhost:6333/health

# Restart Qdrant
docker-compose -f docker/docker-compose.yml restart qdrant
```

**Ollama model not found**
```bash
# List available models
ollama list

# Pull required model
ollama pull llama3.2:3b
```

**Out of memory errors**
```bash
# Switch to Eco profile
export RAG_PROFILE=eco

# Or reduce context budget
export RAG_MAX_CONTEXT_TOKENS=2000
```

**Slow response times**
- Check system resources with `htop` or Task Manager
- Switch to Performance profile if you have >8GB RAM
- Consider using a smaller LLM model

### Logs

Application logs are stored in `~/RAGApp/logs/app.jsonl` in structured JSON format.

Enable debug logging:
```bash
export RAG_DEBUG=true
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Qdrant](https://qdrant.tech/) - Vector similarity search engine
- [Ollama](https://ollama.ai/) - Local LLM runtime
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Python web framework
- [React](https://react.dev/) - Frontend framework
- [Sentence Transformers](https://www.sbert.net/) - State-of-the-art embeddings

---

**Built with â¤ï¸ for privacy-conscious users who want to keep their documents local.**
